{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport glob\nimport pathlib\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchtext.vocab import build_vocab_from_iterator","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:13:15.922846Z","iopub.execute_input":"2022-06-20T10:13:15.923492Z","iopub.status.idle":"2022-06-20T10:13:16.688518Z","shell.execute_reply.started":"2022-06-20T10:13:15.923403Z","shell.execute_reply":"2022-06-20T10:13:16.687721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:13:20.145093Z","iopub.execute_input":"2022-06-20T10:13:20.145617Z","iopub.status.idle":"2022-06-20T10:13:20.18388Z","shell.execute_reply.started":"2022-06-20T10:13:20.14558Z","shell.execute_reply":"2022-06-20T10:13:20.182994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/input/bbc-news-summary/BBC News Summary/Summaries')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:13:24.845443Z","iopub.execute_input":"2022-06-20T10:13:24.846269Z","iopub.status.idle":"2022-06-20T10:13:24.856852Z","shell.execute_reply.started":"2022-06-20T10:13:24.846228Z","shell.execute_reply":"2022-06-20T10:13:24.855973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles_path = '../input/bbc-news-summary/BBC News Summary/News Articles'\nsummaries_path ='../input/bbc-news-summary/BBC News Summary/Summaries'\ncategories_list = ['politics', 'sport', 'tech', 'entertainment', 'business']","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:13:37.840121Z","iopub.execute_input":"2022-06-20T10:13:37.840391Z","iopub.status.idle":"2022-06-20T10:13:37.844605Z","shell.execute_reply.started":"2022-06-20T10:13:37.84036Z","shell.execute_reply":"2022-06-20T10:13:37.843842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_files_from_folders(articles_path,summaries_path,categories_list =['tech','sport'], encoding = \"ISO-8859-1\"):\n    articles = []\n    summaries = []\n    categories = []\n    for category in categories_list:\n        article_paths =  glob.glob(os.path.join(articles_path , category , '*.txt'), recursive=True)\n        summary_paths =  glob.glob(os.path.join(summaries_path , category , '*.txt'), recursive=True)\n\n        print(f'found {len(article_paths)} file in articles/{category} folder, {len(summary_paths)} file in summaries/{category} folder')\n\n        if len(article_paths) != len(summary_paths):\n            print('number of files is not equal') \n            return    \n        for idx_file in range(len(article_paths)):\n            categories.append(category)\n            with open(article_paths[idx_file], mode = 'r', encoding = encoding) as file:\n                articles.append(file.read())\n\n\n            with open(summary_paths[idx_file], mode = 'r', encoding = encoding) as file:\n                 summaries.append(file.read())\n    \n    print(f'total {len(articles)} file in articles folders, {len(summaries)} file in summaries folders')\n    return articles, summaries, categories","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:13:42.255169Z","iopub.execute_input":"2022-06-20T10:13:42.255432Z","iopub.status.idle":"2022-06-20T10:13:42.263942Z","shell.execute_reply.started":"2022-06-20T10:13:42.255401Z","shell.execute_reply":"2022-06-20T10:13:42.263147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"articles, summaries, categories = read_files_from_folders(articles_path, summaries_path, categories_list)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:13:49.619975Z","iopub.execute_input":"2022-06-20T10:13:49.620254Z","iopub.status.idle":"2022-06-20T10:13:55.578418Z","shell.execute_reply.started":"2022-06-20T10:13:49.620223Z","shell.execute_reply":"2022-06-20T10:13:55.575991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'articles':articles,'summaries': summaries, 'categories' : categories},)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:14:12.849898Z","iopub.execute_input":"2022-06-20T10:14:12.850274Z","iopub.status.idle":"2022-06-20T10:14:12.869441Z","shell.execute_reply.started":"2022-06-20T10:14:12.850231Z","shell.execute_reply":"2022-06-20T10:14:12.86869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:14:18.054821Z","iopub.execute_input":"2022-06-20T10:14:18.055537Z","iopub.status.idle":"2022-06-20T10:14:18.068641Z","shell.execute_reply.started":"2022-06-20T10:14:18.055496Z","shell.execute_reply":"2022-06-20T10:14:18.067791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nspecial_tokens = {'pad_token':'<|pad|>','sep_token':'<|sep|>'}\nnum_add_toks = tokenizer.add_special_tokens(special_tokens)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:14:42.67471Z","iopub.execute_input":"2022-06-20T10:14:42.674973Z","iopub.status.idle":"2022-06-20T10:14:47.116793Z","shell.execute_reply.started":"2022-06-20T10:14:42.674933Z","shell.execute_reply":"2022-06-20T10:14:47.115991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop = []\n\nfor i in range(0, len(df)):\n    if((len(tokenizer.encode((df.iloc[i]).articles)) + len(tokenizer.encode((df.iloc[i]).summaries))) >= 1024):\n        to_drop.append(i)\n        #print(\"ok\")\n    \ndf = df.drop(df.index[to_drop]) ","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:14:49.735597Z","iopub.execute_input":"2022-06-20T10:14:49.744508Z","iopub.status.idle":"2022-06-20T10:15:04.942244Z","shell.execute_reply.started":"2022-06-20T10:14:49.744462Z","shell.execute_reply":"2022-06-20T10:15:04.941485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:15:47.160301Z","iopub.execute_input":"2022-06-20T10:15:47.160782Z","iopub.status.idle":"2022-06-20T10:15:47.174695Z","shell.execute_reply.started":"2022-06-20T10:15:47.160741Z","shell.execute_reply":"2022-06-20T10:15:47.173674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.encode(\"Hello world my name\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset, DataLoader","metadata":{}},{"cell_type":"code","source":"class NewsData(Dataset):\n    def __init__(self,dataframe, tokenizer):\n        super(NewsData, self).__init__()\n        self.df = dataframe\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):      \n        text = self.tokenizer.encode(self.tokenizer.pad_token)*1024\n        content = self.tokenizer.encode((df.iloc[index]).articles) + self.tokenizer.encode(self.tokenizer.sep_token) + self.tokenizer.encode((df.iloc[index]).summaries)\n        text[:len(content)] = content\n        text = torch.tensor(text)\n        sample = {'article': text, 'sum_idx': len(self.tokenizer.encode(df.iloc[index].articles))}\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:16:13.416295Z","iopub.execute_input":"2022-06-20T10:16:13.416565Z","iopub.status.idle":"2022-06-20T10:16:13.423889Z","shell.execute_reply.started":"2022-06-20T10:16:13.416534Z","shell.execute_reply":"2022-06-20T10:16:13.422921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(df.iloc[13]).articles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"news_dataset = NewsData(df,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:16:21.15015Z","iopub.execute_input":"2022-06-20T10:16:21.150828Z","iopub.status.idle":"2022-06-20T10:16:21.154349Z","shell.execute_reply.started":"2022-06-20T10:16:21.150787Z","shell.execute_reply":"2022-06-20T10:16:21.153438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"#model = Transformer(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads,\n#                   num_encoder_layers, num_decoder_layers, froward_expansion, dropout, max_len, device).to(device)\nfrom transformers import GPT2LMHeadModel\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\nmodel.resize_token_embeddings(len(tokenizer))\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:16:26.614855Z","iopub.execute_input":"2022-06-20T10:16:26.615141Z","iopub.status.idle":"2022-06-20T10:16:32.778407Z","shell.execute_reply.started":"2022-06-20T10:16:26.615101Z","shell.execute_reply":"2022-06-20T10:16:32.776612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import argparse\nfrom datetime import datetime\nimport os\nimport time\n\nimport numpy as np\nfrom transformers import GPT2LMHeadModel,AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom tqdm import tnrange, tqdm_notebook\nimport random\n\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--lr\",default=5e-5, type=float, help=\"learning rate\")\nparser.add_argument(\"--seed\",default=42, type=int,  help=\"seed to replicate results\")\nparser.add_argument(\"--n_gpu\",default=1, type=int,  help=\"no of gpu available\")\nparser.add_argument(\"--gradient_accumulation_steps\",default=2, type=int, help=\"gradient_accumulation_steps\")\nparser.add_argument(\"--batch_size\",default=1, type=int,  help=\"batch_size\")\nparser.add_argument(\"--num_workers\",default=4, type=int,  help=\"num of cpus available\")\nparser.add_argument(\"--device\",default=torch.device('cuda'), help=\"torch.device object\")\nparser.add_argument(\"--num_train_epochs\",default=1, type=int,  help=\"no of epochs of training\")\nparser.add_argument(\"--output_dir\",default='./output', type=str,  help=\"path to save evaluation results\")\nparser.add_argument(\"--model_dir\",default='./weights', type=str,  help=\"path to save trained model\")\nparser.add_argument(\"--max_grad_norm\",default=1.0, type=float, help=\"max gradient norm.\")\nparser.add_argument(\"--root_dir\",default='./CNN/gpt2_1024_data', type=str, help=\"location of json dataset.\")\nparser.add_argument(\"--ids_file\",default='./CNN/ids.json', type=str, help=\"location of train, valid and test file indexes\")\nargs = parser.parse_args([])\nprint(args)","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:16:45.03991Z","iopub.execute_input":"2022-06-20T10:16:45.040198Z","iopub.status.idle":"2022-06-20T10:16:45.114407Z","shell.execute_reply.started":"2022-06-20T10:16:45.040167Z","shell.execute_reply":"2022-06-20T10:16:45.113639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_arr = []\ndef train(args, model, tokenizer, train_dataset, ignore_index):\n    \"\"\" Trains GPT2 model and logs necessary details.\n        Args:\n            args: dict that contains all the necessary information passed by user while training\n            model: finetuned gpt/gpt2 model\n            tokenizer: GPT/GPT2 tokenizer\n            train_dataset: GPT21024Dataset object for training data\n            ignore_index: token not considered in loss calculation\n    \"\"\"\n    writer = SummaryWriter('./output/logs')\n    train_sampler = RandomSampler(train_dataset)\n    train_dl = DataLoader(train_dataset,sampler=train_sampler,batch_size=args.batch_size,num_workers=args.num_workers)\n    loss_fct = CrossEntropyLoss(ignore_index=ignore_index) #ignores padding token for loss calculation\n    optimizer = AdamW(model.parameters(),lr=args.lr)\n    scheduler = get_linear_schedule_with_warmup(optimizer,100,80000)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    train_iterator = tnrange(int(args.num_train_epochs), desc=\"Epoch\")\n    \n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n        \n    for _ in train_iterator:\n        epoch_iterator = tqdm_notebook(train_dl, desc=\"Training\")\n        for step, batch in enumerate(epoch_iterator):\n            inputs, labels = batch['article'].to(args.device), batch['article'].to(args.device)\n            model.train()\n            logits = model(inputs)[0]\n            # only consider loss on reference summary just like seq2seq models\n            shift_logits = logits[..., batch['sum_idx']:-1, :].contiguous()\n            shift_labels = labels[..., batch['sum_idx']+1:].contiguous()\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n            loss = loss/args.gradient_accumulation_steps\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n                writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n                writer.add_scalar('loss', (tr_loss - logging_loss)/args.gradient_accumulation_steps, global_step)\n                logging_loss = tr_loss\n                print(\"loss:\", loss.item(), end='\\n\\n')\n                loss_arr.append(loss.item())\n                if (step + 1)/args.gradient_accumulation_steps == 1.0:\n                \tprint('After 1st update: ', end='\\n\\n')                \n                \n            if (step + 1) % (10*args.gradient_accumulation_steps) == 0:\n                generate_sample(train_dataset, tokenizer, model, num=2, eval_step=False,device=args.device)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:17:26.542776Z","iopub.execute_input":"2022-06-20T10:17:26.543052Z","iopub.status.idle":"2022-06-20T10:17:26.557993Z","shell.execute_reply.started":"2022-06-20T10:17:26.543017Z","shell.execute_reply":"2022-06-20T10:17:26.557253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.plot(loss_arr,'r-')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:31:48.810784Z","iopub.execute_input":"2022-06-20T10:31:48.811054Z","iopub.status.idle":"2022-06-20T10:31:49.046508Z","shell.execute_reply.started":"2022-06-20T10:31:48.811024Z","shell.execute_reply":"2022-06-20T10:31:49.045832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ignore_idx = tokenizer.pad_token_id\nstart = time.time()\ntrain(args, model, tokenizer, news_dataset, ignore_idx)\nprint('total time: ', (time.time()-start)/60, \" minutes\", end='\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:17:37.14986Z","iopub.execute_input":"2022-06-20T10:17:37.15013Z","iopub.status.idle":"2022-06-20T10:20:58.717334Z","shell.execute_reply.started":"2022-06-20T10:17:37.150101Z","shell.execute_reply":"2022-06-20T10:20:58.716025Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_sample(news_dataset, tokenizer, model, num=2, eval_step=False,device=args.device)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## To Assess Outputs","metadata":{}},{"cell_type":"code","source":"def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    if top_k > 0:\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n    return logits\n\n\ndef sample_seq(model, context, length, device, temperature=1, top_k=0, top_p=0.0):\n    context = torch.tensor(context, dtype=torch.long, device=device)\n    context = context.unsqueeze(0)\n    generated = context\n    with torch.no_grad():  \n        for _ in tnrange(length):\n            inputs = {'input_ids': generated}\n            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet (cached hidden-states)\n            next_token_logits = outputs[0][0, -1, :] / temperature\n            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)\n    return generated\n\n\ndef beam_search(model, context, length, beam_size, device, temperature=1):\n    context = torch.tensor(context, dtype=torch.long, device=device)\n    context = context.unsqueeze(0)\n    with torch.no_grad():  \n        inputs = {'input_ids': context}\n        outputs = model(**inputs) \n        next_token_logits = outputs[0][0, -1, :] / temperature\n        next_token_probs = F.softmax(next_token_logits)\n        scores, indices = torch.topk(next_token_probs, beam_size)\n        indices = indices.tolist()\n        sequences = [[c] for c in indices]\n        for _ in tnrange(length-1):\n            logits = torch.zeros(beam_size*len(next_token_logits))\n            for j in range(len(sequences)):\n                new_generated = torch.cat((context,torch.tensor([sequences[j]], dtype=torch.long, device=device)),dim=1)\n                inputs = {'input_ids': new_generated}\n                outputs = model(**inputs) \n                next_token_logits = outputs[0][0, -1, :] / temperature\n                next_token_probs = F.softmax(next_token_logits)\n                start, stop = j*len(next_token_logits), (j+1)*len(next_token_logits)\n                logits[start:stop] = scores[j]*next_token_probs\n            scores, new_logits_indices = torch.topk(logits,beam_size)\n            logits = (new_logits_indices%50259).tolist()\n            for j in range(len(sequences)):\n                sequences[j] = sequences[j]+[logits[j]]\n    return scores, sequences\n\n\ndef generate_beam_sample(data, tokenizer, model, num=1, length=100, beam_size=3, device=torch.device('cuda')):\n    for i in range(num):\n        sample = data.__getitem__(i)\n        idx = sample['sum_idx']\n        context = sample['article'][:idx].tolist()\n        summary = sample['article'][idx+1:][:100].tolist()\n        scores, sequences = beam_search(model, context, length, beam_size, device)\n        #print('new_article', end='\\n\\n')\n        #print(tokenizer.decode(context[:-1]), end='\\n\\n')\n        print('actual_summary', end='\\n\\n')\n        print(tokenizer.decode(summary), end='\\n\\n')\n        for i in range(len(sequences)):\n            text = tokenizer.convert_ids_to_tokens(sequences[i],skip_special_tokens=True)\n            text = tokenizer.convert_tokens_to_string(text)  \n            print(\"generated_summary-{} and Score is {}.\".format(i+1, scores[i]), end='\\n\\n')\n            print(text, end='\\n\\n')\n\n\ndef generate_sample(data, tokenizer, model, num=1, eval_step=False, length=100, temperature=1, top_k=10, top_p=0.5, device=torch.device('cuda')):\n    for i in range(num):\n        sample = data.__getitem__(i)\n        idx = sample['sum_idx']\n        context = ((sample['article'])[:idx+1]).tolist()\n        summary = sample['article'][idx+1:][:100].tolist()\n        generated_text = sample_seq(model, context, length, device, temperature, top_k, top_p)\n        generated_text = generated_text[0, len(context):].tolist()\n        text = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\n        text = tokenizer.convert_tokens_to_string(text)\n        if eval_step==False:\n            print('new_article', end='\\n\\n')\n            print(tokenizer.decode(context), end='\\n\\n')\n            print(\"generated_summary\", end='\\n\\n')\n            print(text, end='\\n\\n')\n            print('actual_summary', end='\\n\\n')\n            print(tokenizer.decode(summary), end='\\n\\n')\n        else:\n            print(tokenizer.decode(context), end='\\n\\n')\n            print(\"generated_summary\", end='\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-20T10:17:33.333183Z","iopub.execute_input":"2022-06-20T10:17:33.333433Z","iopub.status.idle":"2022-06-20T10:17:33.364771Z","shell.execute_reply.started":"2022-06-20T10:17:33.333403Z","shell.execute_reply":"2022-06-20T10:17:33.363853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = news_dataset.__getitem__(1)\nidx = sample['sum_idx']\ncontext = ((sample['article'])[:idx]).tolist()\nlen(context)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentence = ''' In first half of 20th century, factions of Indian National Congress continued to remain getting identified with \"Hindu politics\" and ideas of a Hindu nation. The word \"Hindu\", throughout history, had been used as an inclusive description that lacked a definition and was used to refer to the native traditions and people of India. It was only in the late 18th century that the word \"Hindu\" came to be used extensively with religious connotation, while still being used as a synecdoche describing the indigenous traditions. Hindu nationalist ideologies and political languages were very diverse both linguistically and socially. Since Hinduism does not represent an identifiable religious group, the terms such as 'Hindu nationalism', 'Hindu', are considered problematic in the case of religious and nationalism discourse. As Hindus were identifiable as a homogeneous community, some individual Congress leaders were able to induce a symbolism with \"Hindu\" meaning inside the general stance of a secular nationalism.[12][13]\n\nThe diversity of Indian cultural groups and moderate positions of Hindu nationalism have sometimes made it regarded as cultural nationalism than a religious one.'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = (tokenizer.encode(test_sentence))\ngenerated_text = sample_seq(model, context, 100, device, 1, 10, 0.5)\ngenerated_text = generated_text[0, len(context):].tolist()\ntext = tokenizer.convert_ids_to_tokens(generated_text,skip_special_tokens=True)\ntext = tokenizer.convert_tokens_to_string(text)\nprint(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install newspaper3k","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pygooglenews ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nimport os\n\n## Setting to use the 0th GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nsummarizer = pipeline(\"summarization\")\n\nfrom newspaper import Article\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary_text = summarizer(toi_article.text[:3423], max_length=200, min_length=150, do_sample=False)[0]['summary_text']\nprint(summary_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_sentence = ''' In first half of 20th century, factions of Indian National Congress continued to remain getting identified with \"Hindu politics\" and ideas of a Hindu nation. The word \"Hindu\", throughout history, had been used as an inclusive description that lacked a definition and was used to refer to the native traditions and people of India. It was only in the late 18th century that the word \"Hindu\" came to be used extensively with religious connotation, while still being used as a synecdoche describing the indigenous traditions. Hindu nationalist ideologies and political languages were very diverse both linguistically and socially. Since Hinduism does not represent an identifiable religious group, the terms such as 'Hindu nationalism', 'Hindu', are considered problematic in the case of religious and nationalism discourse. As Hindus were identifiable as a homogeneous community, some individual Congress leaders were able to induce a symbolism with \"Hindu\" meaning inside the general stance of a secular nationalism.\nThe diversity of Indian cultural groups and moderate positions of Hindu nationalism have sometimes made it regarded as cultural nationalism than a religious one.'''\n\nsummary_text = summarizer(test_sentence, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\nprint(summary_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pygooglenews import GoogleNews\n\nsummaries = []\nfrom pygooglenews import GoogleNews\ngn = GoogleNews()\nsearch = gn.search(\"modi\")\nfor item in search[\"entries\"]:\n    l = item[\"links\"]\n    toi_article = Article(l, language=\"en\") # en for English\n    toi_article.download()\n    toi_article.parse()\n    summaries.append(summarizer(toi_article.text[:3423], max_length=200, min_length=150, do_sample=False)[0]['summary_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}